{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63160324",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "63160324",
    "outputId": "b135c83d-9e73-463e-e561-5812e354a091"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 17:15:59.770708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-18 17:15:59.983464: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-18 17:16:00.709012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-18 17:16:00.709125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-18 17:16:00.709134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/popfabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/popfabio/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/popfabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/popfabio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/popfabio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Carregue as bibliotecas necess√°rias\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Ferramenta de NLP\n",
    "from textblob import TextBlob\n",
    "# Ferramenta para visualiza√ß√£o dos Dados\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Ferramentas do pacote NLTK\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize , download , pos_tag\n",
    "# Ferramentas d lib padr√£o python3\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "# Download de mais Ferramentas da biblioteca do NLTK\n",
    "download(['punkt','averaged_perceptron_tagger','stopwords','wordnet','omw-1.4'])\n",
    "stopwords = stopwords.words('portuguese')\n",
    "# Para esse projeto usaremos a T√©cnica de Lematiza√ß√£o, pois queremos ter a certeza de que a forma reduzida de uma palavra existe (radical)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6cec008",
   "metadata": {
    "id": "b6cec008"
   },
   "outputs": [],
   "source": [
    "database = {\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"tag\": \"welcome\",\n",
    "      \"patterns\": [\"Oi\",\"Oi, bom dia\",\"Oi, boa tarde\", \"bom dia\", \"boa tarde\", \"boa noite\", \"oi, boa noite\", \"ol√°, boa noite\", \"oiiiii\", \"Ol√°\",\"oiii, como vai?\",\"opa, tudo bem?\"],\n",
    "      \"responses\": [\"Ol√°, serei seu assistente virtual, em que posso te ajudar?\",\"Salve, qual foi ?\", \"Manda pro pai, Lan√ßa a braba\", \"No que posso te ajudar ?\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"my_classes\",\n",
    "      \"patterns\": [\"Quais s√£o as minhas mat√©rias ?\",\"Quais s√£o as minhas mat√©rias de hoje ? \",\"Quais s√£o as minhas disciplinas de hoje ? \", \"Que aulas eu tenho Hoje\",\"me fale minhas turmas\", \"que sala eu devo ir?\", \"Qual minha Sala ?\",\"quais as minhas turmas ?\"],\n",
    "      \"responses\": [\"Entendi, voc√™ deseja saber suas salas\",\"Voc√™ deseja saber suas salas ?\", \"Ah, voc√™ quer saber qual sala ? \", \"Suas Aulas ?\"],\n",
    "      \"context\": [\"RA\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"anything_else\",\n",
    "      \"patterns\": [],\n",
    "      \"responses\": [\"Desculpa, n√£o entendi o que voc√™ falou, tente novamente!\",\"N√£o compreendi a sua solicita√ß√£o, talvez eu possa te ajudar\"],\n",
    "      \"context\": [\"\"]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24154c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quais s√£o as minhas mat√©rias ?',\n",
       " 'Quais s√£o as minhas mat√©rias de hoje ? ',\n",
       " 'Quais s√£o as minhas disciplinas de hoje ? ',\n",
       " 'Que aulas eu tenho Hoje',\n",
       " 'me fale minhas turmas',\n",
       " 'que sala eu devo ir?',\n",
       " 'Qual minha Sala ?',\n",
       " 'quais as minhas turmas ?',\n",
       " 'meu ra √© Quais s√£o as minhas mat√©rias ?',\n",
       " 'meu ra √© Quais s√£o as minhas mat√©rias de hoje ? ',\n",
       " 'meu ra √© Quais s√£o as minhas disciplinas de hoje ? ',\n",
       " 'meu ra √© Que aulas eu tenho Hoje',\n",
       " 'meu ra √© me fale minhas turmas',\n",
       " 'meu ra √© que sala eu devo ir?',\n",
       " 'meu ra √© Qual minha Sala ?',\n",
       " 'meu ra √© quais as minhas turmas ?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_classes_triggers = database['intents'][1]['patterns'] + [ \"meu ra √© \" + sentence for sentence in database['intents'][1]['patterns'] ]\n",
    "\n",
    "display(my_classes_triggers)\n",
    "database['intents'][1]['patterns'] = my_classes_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f54fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "database['intents'][1]['patterns'] = ['Quais s√£o as minhas mat√©rias ?',  'Quais s√£o as minhas mat√©rias de hoje ? ',  'Quais s√£o as minhas disciplinas de hoje ? ',  'Que aulas eu tenho Hoje',  'me fale minhas turmas',  'que sala eu devo ir?',  'Qual minha Sala ?',  'quais as minhas turmas ?',  'meu ra √© Quais s√£o as minhas mat√©rias ?',  'meu ra √© Quais s√£o as minhas mat√©rias de hoje ? ',  'meu ra √© Quais s√£o as minhas disciplinas de hoje ? ',  'meu ra √© Que aulas eu tenho Hoje',  'meu ra √© me fale minhas turmas',  'meu ra √© que sala eu devo ir?',  'meu ra √© Qual minha Sala ?',  'meu ra √© quais as minhas turmas ?', 'Quero saber minhas turmas', 'Quero saber minhas salas', 'Quero saber minhas disciplinas', 'Quero saber minhas turmas meu ra √©', 'Quero saber minhas salas meu ra √©', 'Quero saber minhas disciplinas meu ra √©', 'meu ra √© Quero saber minhas turmas', 'meu ra √© Quero saber minhas salas', 'meu ra √© Quero saber minhas disciplinas', 'Que aula hoje ?', 'Que turma hoje ?', 'Que disciplina hoje ?', 'Que aula agora ?', 'Que turma agora ?', 'Que disciplina agora ?', 'Que mat√©ria hoje ?', 'Que m√°teria agora ?', 'Quais minhas pr√≥ximas aulas ?', 'Qual minha pr√≥xima turma ?', 'Qual minha pr√≥xima sala ?', 'Quero saber minha pr√≥xima aula', 'Quero saber minha pr√≥xima turma', 'Quero saber minha pr√≥xima sala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbe70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "database['intents'][0]['patterns'] = 7*database['intents'][0]['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e722582a",
   "metadata": {
    "id": "e722582a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def limpa_emotes(texto):\n",
    "  \"\"\"\n",
    "    Fun√ß√£o para Limpar emotes dos Coment√°rios\n",
    "    \n",
    "    Exemplo :  ü§î ?\n",
    "  \"\"\"\n",
    "  emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "        \"]+\",\n",
    "                      re.UNICODE)\n",
    "  return re.sub(emoj, '', texto)\n",
    "\n",
    "def limpa_urls(texto) : \n",
    "  \"\"\"\n",
    "  Fun√ß√£o para Limpar emotes dos Coment√°rios\n",
    "    \n",
    "    Exemplo :  Ol√° segue https://www.youtube.com/playlist?list=PLYItvall0TqLBLt6oXFVBaloU7-xZsV-v\n",
    "  \"\"\"\n",
    "  return re.sub('http\\S+|www\\S+|https\\S+', '', texto)\n",
    "\n",
    "def limpa_pontos(texto) : \n",
    "  \"\"\"\n",
    "    Fun√ß√£o para limpar pontua√ß√µes \n",
    "    Exemplo : eu pago $2.99 por esse aplicativo ... ele deve funcionar ! \n",
    "  \"\"\"\n",
    "  return re.sub(r'[^\\w]', ' ', texto)\n",
    "\n",
    "def limpa_asc2_emotes(texto) :\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para desenhos em asc2 \n",
    "    Exemplo :      _____\n",
    "                  /     \\\n",
    "                /- (*) |*)\\\n",
    "                |/\\.  _>/\\|\n",
    "                    \\__/  \n",
    "    \"\"\"\n",
    "    return re.sub('[\\.\\,\\;\\!\\?\\*\\$]','',texto)\n",
    "\n",
    "def limpeza_simples(texto) : \n",
    "  \"\"\"\n",
    "    Fun√ß√£o que aplica limpa_emotes, limpa_urls, limpa_numeros, limpa_asc2_emotes\n",
    "    a um coment√°rio\n",
    "  \"\"\"\n",
    "  texto = texto.lower()\n",
    "  #texto = str( [ word for word in list(texto) if len(word) >= 4] )\n",
    "  #print(texto)\n",
    "  texto = limpa_urls(texto)\n",
    "  texto = limpa_pontos(texto)\n",
    "  texto = limpa_asc2_emotes(texto)\n",
    "  texto = limpa_emotes(texto)\n",
    "  return texto\n",
    "\n",
    "\n",
    "def preproc_tokenizer(comment, tokenizer,stopwords=stopwords):\n",
    "    # tokeniza os twtes, usando o tokenizador para essa rede social\n",
    "    tokens_processados = tokenizer.tokenize(comment) \n",
    "    # remove as stopwords\n",
    "    tokens_processados = [word for word in tokens_processados if word not in stopwords and len(word) > 3]\n",
    "    # remove pontua√ß√£o\n",
    "    tokens_processados = [word for word in tokens_processados if word not in string.punctuation]\n",
    "    # aplica Lematiza√ß√£o\n",
    "    tokens_processados = [wnl.lemmatize(word) for word in tokens_processados] \n",
    "\n",
    "    return tokens_processados\n",
    "\n",
    "\n",
    "def preproc_entidades_nomeadas(tokens_processados):\n",
    "    return pos_tag(tokens_processados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20235cf7",
   "metadata": {
    "id": "20235cf7",
    "outputId": "e3c74508-8599-4c72-98f3-90122d8078d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'its', \"i'm\", 'that', 'own', 'do', 'have', 'same', 'then', 'he', 'him', \"it's\", 'did', \"they've\", 'by', 'hence', 'which', 'hers', 'for', 'those', \"when's\", 'would', 'some', \"i'll\", \"we'll\", 'of', 'was', 'should', 'been', 'how', 'down', \"mustn't\", \"shouldn't\", 'doing', 'any', 'and', \"weren't\", \"you'll\", 'cannot', 'until', 'am', 'our', 'yours', \"what's\", 'over', 'r', \"she'll\", 'both', 'an', 'i', 'herself', 'it', 'no', 'further', 'theirs', 'she', \"let's\", 'these', \"there's\", 'with', 'could', 'who', 'whom', 'after', \"we've\", \"i'd\", 'very', \"you've\", 'them', 'at', 'in', 'when', 'to', 'few', 'through', \"they'd\", \"wouldn't\", 'me', 'too', 'does', 'shall', 'into', \"can't\", 'like', 'therefore', \"shan't\", 'up', 'your', \"isn't\", \"haven't\", 'but', 'are', 'not', 'ours', \"wasn't\", 'else', 'himself', 'since', \"couldn't\", \"who's\", 'also', 'between', 'can', \"hadn't\", 'because', \"didn't\", 'why', \"they're\", \"they'll\", 'is', 'here', 'such', \"where's\", \"we'd\", 'itself', 'once', 'nor', \"we're\", \"she'd\", \"you'd\", 'only', \"don't\", \"he'd\", \"she's\", 'other', 'than', 'the', 'before', 'most', 'themselves', 'during', \"he's\", \"why's\", 'they', 'above', \"that's\", 'his', 'while', 'however', 'again', \"i've\", 'having', 'had', 'from', 'so', 'k', 'we', 'otherwise', \"won't\", \"you're\", \"he'll\", 'her', 'each', 'their', 'yourselves', 'be', \"doesn't\", 'has', 'if', 'or', 'off', \"aren't\", 'get', 'about', 'what', 'yourself', 'against', 'you', 'on', 'just', 'www', 'my', 'ourselves', 'where', 'ever', \"hasn't\", 'out', 'there', 'com', 'below', \"here's\", \"how's\", 'under', 'as', 'were', 'more', 'myself', 'ought', 'all', 'this', 'being', 'http', 'a'}\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "documents = []\n",
    "intents = database\n",
    "\n",
    "classes = [i['tag'] for i in intents['intents']]\n",
    "ignore_words = STOPWORDS\n",
    "\n",
    "print(ignore_words)\n",
    "#intents = json.loads(open('intents.json').read())\n",
    "\n",
    "# percorremos nosso array de objetos\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "\n",
    "        # adiciona aos documentos para identificarmos a tag para a mesma\n",
    "        documents.append((word, intent['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb45ae33",
   "metadata": {
    "id": "bb45ae33",
    "outputId": "905b8747-291e-4b08-faad-441ef3512586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Oi'], 'welcome'),\n",
       " (['Oi', ',', 'bom', 'dia'], 'welcome'),\n",
       " (['Oi', ',', 'boa', 'tarde'], 'welcome'),\n",
       " (['bom', 'dia'], 'welcome'),\n",
       " (['boa', 'tarde'], 'welcome'),\n",
       " (['boa', 'noite'], 'welcome'),\n",
       " (['oi', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['ol√°', ',', 'boa', 'noite'], 'welcome'),\n",
       " (['oiiiii'], 'welcome'),\n",
       " (['Ol√°'], 'welcome'),\n",
       " (['oiii', ',', 'como', 'vai', '?'], 'welcome'),\n",
       " (['opa', ',', 'tudo', 'bem', '?'], 'welcome'),\n",
       " (['Quais', 's√£o', 'as', 'minhas', 'mat√©rias', '?'], 'my_classes'),\n",
       " (['Quais', 's√£o', 'as', 'minhas', 'mat√©rias', 'de', 'hoje', '?'],\n",
       "  'my_classes'),\n",
       " (['Quais', 's√£o', 'as', 'minhas', 'disciplinas', 'de', 'hoje', '?'],\n",
       "  'my_classes'),\n",
       " (['Que', 'aulas', 'eu', 'tenho', 'Hoje'], 'my_classes'),\n",
       " (['me', 'fale', 'minhas', 'turmas'], 'my_classes'),\n",
       " (['que', 'sala', 'eu', 'devo', 'ir', '?'], 'my_classes'),\n",
       " (['Qual', 'minha', 'Sala', '?'], 'my_classes'),\n",
       " (['quais', 'as', 'minhas', 'turmas', '?'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Quais', 's√£o', 'as', 'minhas', 'mat√©rias', '?'],\n",
       "  'my_classes'),\n",
       " (['meu',\n",
       "   'ra',\n",
       "   '√©',\n",
       "   'Quais',\n",
       "   's√£o',\n",
       "   'as',\n",
       "   'minhas',\n",
       "   'mat√©rias',\n",
       "   'de',\n",
       "   'hoje',\n",
       "   '?'],\n",
       "  'my_classes'),\n",
       " (['meu',\n",
       "   'ra',\n",
       "   '√©',\n",
       "   'Quais',\n",
       "   's√£o',\n",
       "   'as',\n",
       "   'minhas',\n",
       "   'disciplinas',\n",
       "   'de',\n",
       "   'hoje',\n",
       "   '?'],\n",
       "  'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Que', 'aulas', 'eu', 'tenho', 'Hoje'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'me', 'fale', 'minhas', 'turmas'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'que', 'sala', 'eu', 'devo', 'ir', '?'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Qual', 'minha', 'Sala', '?'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'quais', 'as', 'minhas', 'turmas', '?'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'turmas'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'salas'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'disciplinas'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'turmas', 'meu', 'ra', '√©'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'salas', 'meu', 'ra', '√©'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minhas', 'disciplinas', 'meu', 'ra', '√©'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Quero', 'saber', 'minhas', 'turmas'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Quero', 'saber', 'minhas', 'salas'], 'my_classes'),\n",
       " (['meu', 'ra', '√©', 'Quero', 'saber', 'minhas', 'disciplinas'], 'my_classes'),\n",
       " (['Que', 'aula', 'hoje', '?'], 'my_classes'),\n",
       " (['Que', 'turma', 'hoje', '?'], 'my_classes'),\n",
       " (['Que', 'disciplina', 'hoje', '?'], 'my_classes'),\n",
       " (['Que', 'aula', 'agora', '?'], 'my_classes'),\n",
       " (['Que', 'turma', 'agora', '?'], 'my_classes'),\n",
       " (['Que', 'disciplina', 'agora', '?'], 'my_classes'),\n",
       " (['Que', 'mat√©ria', 'hoje', '?'], 'my_classes'),\n",
       " (['Que', 'm√°teria', 'agora', '?'], 'my_classes'),\n",
       " (['Quais', 'minhas', 'pr√≥ximas', 'aulas', '?'], 'my_classes'),\n",
       " (['Qual', 'minha', 'pr√≥xima', 'turma', '?'], 'my_classes'),\n",
       " (['Qual', 'minha', 'pr√≥xima', 'sala', '?'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minha', 'pr√≥xima', 'aula'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minha', 'pr√≥xima', 'turma'], 'my_classes'),\n",
       " (['Quero', 'saber', 'minha', 'pr√≥xima', 'sala'], 'my_classes')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lematizamos as palavras ignorando os palavras da lista ignore_words\n",
    "words =  [ limpeza_simples(w) for w in words ]\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "\n",
    "# classificamos nossas listas\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# print(words)\n",
    "# print(classes)\n",
    "#print(sorted(documents,key=lambda x:x[1]))\n",
    "# salvamos as palavras e classes nos arquivos pkl\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "display(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4295df60",
   "metadata": {
    "id": "4295df60",
    "outputId": "7a269306-bfe5-4fb3-bcad-951697b843a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Oi', 'Oi , bom dia', 'Oi , boa tarde', 'bom dia', 'boa tarde', 'boa noite', 'oi , boa noite', 'ol√° , boa noite', 'oiiiii', 'Ol√°', 'oiii , como vai ?', 'opa , tudo bem ?', 'Quais s√£o as minhas mat√©rias ?', 'Quais s√£o as minhas mat√©rias de hoje ?', 'Quais s√£o as minhas disciplinas de hoje ?', 'Que aulas eu tenho Hoje', 'me fale minhas turmas', 'que sala eu devo ir ?', 'Qual minha Sala ?', 'quais as minhas turmas ?', 'meu ra √© Quais s√£o as minhas mat√©rias ?', 'meu ra √© Quais s√£o as minhas mat√©rias de hoje ?', 'meu ra √© Quais s√£o as minhas disciplinas de hoje ?', 'meu ra √© Que aulas eu tenho Hoje', 'meu ra √© me fale minhas turmas', 'meu ra √© que sala eu devo ir ?', 'meu ra √© Qual minha Sala ?', 'meu ra √© quais as minhas turmas ?', 'Quero saber minhas turmas', 'Quero saber minhas salas', 'Quero saber minhas disciplinas', 'Quero saber minhas turmas meu ra √©', 'Quero saber minhas salas meu ra √©', 'Quero saber minhas disciplinas meu ra √©', 'meu ra √© Quero saber minhas turmas', 'meu ra √© Quero saber minhas salas', 'meu ra √© Quero saber minhas disciplinas', 'Que aula hoje ?', 'Que turma hoje ?', 'Que disciplina hoje ?', 'Que aula agora ?', 'Que turma agora ?', 'Que disciplina agora ?', 'Que mat√©ria hoje ?', 'Que m√°teria agora ?', 'Quais minhas pr√≥ximas aulas ?', 'Qual minha pr√≥xima turma ?', 'Qual minha pr√≥xima sala ?', 'Quero saber minha pr√≥xima aula', 'Quero saber minha pr√≥xima turma', 'Quero saber minha pr√≥xima sala']\n",
      "{'w e l c o m e', 'm y _ c l a s s e s'}\n"
     ]
    }
   ],
   "source": [
    "corpus = [ \" \".join(pattern[0]) for pattern in documents]\n",
    "print(corpus)\n",
    "\n",
    "labels = set([ \" \".join(pattern[1]) for pattern in documents])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68e9146",
   "metadata": {
    "id": "b68e9146",
    "outputId": "173b6a90-f3f0-4263-be9a-573269992f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79406/954019658.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# inicializamos o treinamento\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for document in documents:\n",
    "    # inicializamos o saco de palavras \n",
    "    bag = []\n",
    "\n",
    "    # listamos as palavras do pattern\n",
    "    pattern_words = document[0]\n",
    "\n",
    "    # lematizamos cada palavra \n",
    "    # na tentativa de representar palavras relacionadas\n",
    "    pattern_words = [lemmatizer.lemmatize( word.lower()) for word in pattern_words]\n",
    "\n",
    "    # criamos nosso conjunto de palavras com 1, \n",
    "    # se a correspond√™ncia de palavras for encontrada no padr√£o     atual\n",
    "    for word in words:\n",
    "        bag.append(1) if word in pattern_words else bag.append(0)\n",
    "\n",
    "    # output_row atuar√° como uma chave para a lista, \n",
    "    # onde a saida ser√° 0 para cada tag e 1 para a tag atual\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "#print(training)\n",
    "# embaralhamos nosso conjunto de treinamentos e transformamos em numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# criamos lista de treino sendo x os patterns e y as inten√ß√µes\n",
    "x = list(training[:, 0])\n",
    "y = list(training[:, 1])\n",
    "\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0d38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6392eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos separar a base em conjuntos de treino e de test aleatorioamente\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ea4a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 47)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39c22815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86594ea",
   "metadata": {
    "id": "a86594ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               6144      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,595\n",
      "Trainable params: 14,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 1s 125ms/step - loss: 0.9061 - precision: 0.9333 - val_loss: 0.5189 - val_precision: 0.9714\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3792 - precision: 0.9620 - val_loss: 0.1754 - val_precision: 0.9750\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0880 - precision: 1.0000 - val_loss: 0.0473 - val_precision: 1.0000\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0122 - precision: 1.0000 - val_loss: 0.0099 - val_precision: 1.0000\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0035 - precision: 1.0000 - val_loss: 0.0018 - val_precision: 1.0000\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.0600e-04 - precision: 1.0000 - val_loss: 3.4823e-04 - val_precision: 1.0000\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 3.0616e-04 - precision: 1.0000 - val_loss: 8.0600e-05 - val_precision: 1.0000\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.1202e-05 - precision: 1.0000 - val_loss: 2.2843e-05 - val_precision: 1.0000\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 7.0943e-04 - precision: 1.0000 - val_loss: 7.6435e-06 - val_precision: 1.0000\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.5627e-05 - precision: 1.0000 - val_loss: 2.8145e-06 - val_precision: 1.0000\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.1435e-05 - precision: 1.0000 - val_loss: 1.2793e-06 - val_precision: 1.0000\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 7.6976e-06 - precision: 1.0000 - val_loss: 6.8618e-07 - val_precision: 1.0000\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 3.0272e-05 - precision: 1.0000 - val_loss: 4.3032e-07 - val_precision: 1.0000\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.3055e-05 - precision: 1.0000 - val_loss: 2.9657e-07 - val_precision: 1.0000\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.6865e-06 - precision: 1.0000 - val_loss: 2.2388e-07 - val_precision: 1.0000\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.0124e-07 - precision: 1.0000 - val_loss: 1.8318e-07 - val_precision: 1.0000\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.2735e-06 - precision: 1.0000 - val_loss: 1.4538e-07 - val_precision: 1.0000\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.1399e-06 - precision: 1.0000 - val_loss: 1.3375e-07 - val_precision: 1.0000\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.7677e-06 - precision: 1.0000 - val_loss: 1.1049e-07 - val_precision: 1.0000\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.9945e-06 - precision: 1.0000 - val_loss: 1.0467e-07 - val_precision: 1.0000\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.6296e-06 - precision: 1.0000 - val_loss: 1.0467e-07 - val_precision: 1.0000\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.0438e-06 - precision: 1.0000 - val_loss: 9.8856e-08 - val_precision: 1.0000\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 6.3674e-07 - precision: 1.0000 - val_loss: 8.7226e-08 - val_precision: 1.0000\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.6761e-06 - precision: 1.0000 - val_loss: 8.4319e-08 - val_precision: 1.0000\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 7.6322e-07 - precision: 1.0000 - val_loss: 8.4319e-08 - val_precision: 1.0000\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.2314e-06 - precision: 1.0000 - val_loss: 8.1411e-08 - val_precision: 1.0000\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.1723e-07 - precision: 1.0000 - val_loss: 8.1411e-08 - val_precision: 1.0000\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.0089e-06 - precision: 1.0000 - val_loss: 8.1411e-08 - val_precision: 1.0000\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.9830e-06 - precision: 1.0000 - val_loss: 8.1411e-08 - val_precision: 1.0000\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.6842e-05 - precision: 1.0000 - val_loss: 8.1411e-08 - val_precision: 1.0000\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.5027e-04 - precision: 1.0000 - val_loss: 7.5596e-08 - val_precision: 1.0000\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.8220e-05 - precision: 1.0000 - val_loss: 6.6873e-08 - val_precision: 1.0000\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.2345e-06 - precision: 1.0000 - val_loss: 6.1058e-08 - val_precision: 1.0000\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.3663e-05 - precision: 1.0000 - val_loss: 5.5243e-08 - val_precision: 1.0000\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.0816e-06 - precision: 1.0000 - val_loss: 5.5243e-08 - val_precision: 1.0000\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.2106e-07 - precision: 1.0000 - val_loss: 5.5243e-08 - val_precision: 1.0000\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.3935e-07 - precision: 1.0000 - val_loss: 5.2336e-08 - val_precision: 1.0000\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.7538e-07 - precision: 1.0000 - val_loss: 5.2336e-08 - val_precision: 1.0000\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.0158e-04 - precision: 1.0000 - val_loss: 4.9428e-08 - val_precision: 1.0000\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.5425e-06 - precision: 1.0000 - val_loss: 4.9428e-08 - val_precision: 1.0000\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.7652e-07 - precision: 1.0000 - val_loss: 4.9428e-08 - val_precision: 1.0000\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6934e-07 - precision: 1.0000 - val_loss: 4.6521e-08 - val_precision: 1.0000\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 7.6613e-07 - precision: 1.0000 - val_loss: 4.6521e-08 - val_precision: 1.0000\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.8494e-07 - precision: 1.0000 - val_loss: 4.6521e-08 - val_precision: 1.0000\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 4.7299e-06 - precision: 1.0000 - val_loss: 4.6521e-08 - val_precision: 1.0000\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.7009e-07 - precision: 1.0000 - val_loss: 4.3613e-08 - val_precision: 1.0000\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 9.5365e-07 - precision: 1.0000 - val_loss: 4.3613e-08 - val_precision: 1.0000\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 23ms/step - loss: 1.3956e-06 - precision: 1.0000 - val_loss: 4.3613e-08 - val_precision: 1.0000\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 8.3591e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 5.9605e-08 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.6084e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.3873e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 8.3444e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 7.6031e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 8.5770e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.1400e-05 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.7132e-07 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 5.7628e-05 - precision: 1.0000 - val_loss: 3.1983e-08 - val_precision: 1.0000\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.2444e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.5304e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.0644e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 6.6437e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4130e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.3280e-05 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 9.5511e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 2.5150e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.3549e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 5.4225e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 2.5005e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.0643e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.2689e-08 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 9.8274e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 9.8999e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.4287e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 3.1111e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.7009e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.0622e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.5223e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.8957e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.3956e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 4.9864e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 5.5243e-08 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 3.6383e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.8899e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 7.6467e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.7572e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 9.3477e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 4.4274e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4043e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.6022e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.5033e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 6.6291e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.6808e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.4290e-06 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 4.5066e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.9573e-07 - precision: 1.0000 - val_loss: 2.9075e-08 - val_precision: 1.0000\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 3.9610e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.7974e-08 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 4.3322e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.7912e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.0045e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 25ms/step - loss: 5.2481e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 6.2948e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.0220e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.5410e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 5.7568e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.0991e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 3.3146e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 5.5496e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.0467e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.3551e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.5005e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 6.6873e-08 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.3842e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.1080e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.0207e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 8.1411e-08 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.9044e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 4.0705e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 3.1547e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 6.0402e-05 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.1848e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 5.0882e-08 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.0229e-05 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 4.4194e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.9282e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.1950e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.2502e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.7718e-05 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 5.8080e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.6229e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.1049e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.6980e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 3.4163e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.2371e-06 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.4859e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 4.7538e-07 - precision: 1.0000 - val_loss: 1.7445e-08 - val_precision: 1.0000\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.5441e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 8.8532e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.5991e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.8991e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.3031e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.7445e-08 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.6893e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.6230e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.4887e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.1863e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.9489e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 9.4495e-08 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.5564e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.8348e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.8089e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.9112e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.5555e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.8379e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 6.1058e-08 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.8785e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 5.7417e-06 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.5311e-05 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.1868e-07 - precision: 1.0000 - val_loss: 1.4538e-08 - val_precision: 1.0000\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.8317e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3229e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4683e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.0353e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.9188e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.8847e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.1557e-05 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 5.4806e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.1723e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.0361e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 7.9957e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.8317e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 1.0263e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 6.7017e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 6.7829e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.9524e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.5132e-05 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4566e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.0657e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 9.1587e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 1.0613e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 9.8527e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.4278e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.6811e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.4973e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 2.9075e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.1111e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.7154e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 5.7545e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 7.9770e-05 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.6168e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.7247e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 6.7308e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.9160e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.1806e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 6.0040e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 9.7403e-08 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.4392e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 6.9110e-06 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 2.9366e-07 - precision: 1.0000 - val_loss: 1.1630e-08 - val_precision: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60b42bb850>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "iShape = (x.shape[1],)\n",
    "oShape = y.shape[1]\n",
    "\n",
    "# the deep learning model\n",
    "ourNewModel = Sequential()\n",
    "ourNewModel.add(Dense(128, input_shape=iShape, activation=\"relu\"))\n",
    "ourNewModel.add(Dropout(0.5))\n",
    "ourNewModel.add(Dense(64, activation=\"relu\"))\n",
    "ourNewModel.add(Dropout(0.3))\n",
    "ourNewModel.add(Dense(oShape, activation = \"softmax\"))\n",
    "md = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "ourNewModel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=md,\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "print(ourNewModel.summary())\n",
    "ourNewModel.fit(X_train, y_train, epochs=200, verbose=1 ,  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93b8d045",
   "metadata": {
    "id": "93b8d045"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def clear_writing(writing):\n",
    "    \"\"\"\n",
    "        Limpa todas as senten√ßas inseridas.\n",
    "    \"\"\"\n",
    "\n",
    "    #tokeniza todas as frases inseridas, lematiza cada uma delas e retorna\n",
    "    sentence_words = nltk.word_tokenize(writing)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "\n",
    "\n",
    "# retorna 0 ou 1 para cada palavra da bolsa de palavras\n",
    "\n",
    "\n",
    "def bag_of_words(writing, words):\n",
    "    \"\"\"\n",
    "        Pega as senten√ßas que s√£o limpas e cria um pacote de palavras que s√£o usadas \n",
    "        para classes de previs√£o que s√£o baseadas nos resultados que obtivemos treinando o modelo.\n",
    "    \"\"\"\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clear_writing(writing)\n",
    "\n",
    "    # cria uma matriz de N palavras\n",
    "    bag = [0]*len(words)\n",
    "    for setence in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == setence:\n",
    "                # atribui 1 no pacote de palavra se a palavra atual estiver na posi√ß√£o da frase\n",
    "                bag[i] = 1\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "\n",
    "def class_prediction(writing, model):\n",
    "    \"\"\"\n",
    "      Faz a previsao do pacote de palavras, usamos como limite de erro 0.25 para evitarmos overfitting\n",
    "      e classificamos esses resultados por for√ßa da probabilidade.\n",
    "    \"\"\"\n",
    "\n",
    "    # filtra as previs√µes abaixo de um limite 0.25\n",
    "    prevision = bag_of_words(writing, words)\n",
    "    response_prediction = model.predict(np.array([prevision]))[0]\n",
    "    results = [[index, response] for index, response in enumerate(response_prediction) if response > 0.25]    \n",
    "    #print(results)\n",
    "    # verifica nas previs√µes se n√£o h√° 1 na lista, se n√£o h√° envia a resposta padr√£o (anything_else) \n",
    "    # ou se n√£o corresponde a margem de erro\n",
    "    if \"1\" not in str(prevision) or len(results) == 0 :\n",
    "        results = [[0, response_prediction[0]]]\n",
    "\n",
    "    # classifica por for√ßa de probabilidade\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    print([{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results])\n",
    "    return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
    "\n",
    "\n",
    "def get_response(intents, intents_json):\n",
    "    \"\"\"\n",
    "        pega a lista gerada e verifica o arquivo json e produz a maior parte das respostas com a maior probabilidade.\n",
    "    \"\"\"\n",
    "    tag = intents[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for idx in list_of_intents:\n",
    "        if idx['tag'] == tag:\n",
    "            # caso as respostas sejam um array contendo mais de uma, \n",
    "            # usamos a fun√ß√£o de random para pegar uma resposta randomica da nossa lista\n",
    "            result = random.choice(idx['responses'])\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37011909",
   "metadata": {
    "id": "37011909"
   },
   "outputs": [],
   "source": [
    "# def class_prediction(writting,model):\n",
    "#     sentence_words = nltk.word_tokenize(writting)\n",
    "#     write =  [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "#     rep_tf = tfidf.transform(write)\n",
    "#     response_prediction = model.predict(rep_tf)[0]\n",
    "#     results = [[index, response] for index, response in enumerate(response_prediction) if response > 0.25] \n",
    "#     print(results)\n",
    "#         # classifica por for√ßa de probabilidade\n",
    "#     results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
    "\n",
    "# def get_response(intents, intents_json):\n",
    "#     \"\"\"\n",
    "#         pega a lista gerada e verifica o arquivo json e produz a maior parte das respostas com a maior probabilidade.\n",
    "#     \"\"\"\n",
    "#     tag = intents[0]['intent']\n",
    "#     list_of_intents = intents_json['intents']\n",
    "#     for idx in list_of_intents:\n",
    "#         if idx['tag'] == tag:\n",
    "#             # caso as respostas sejam um array contendo mais de uma, \n",
    "#             # usamos a fun√ß√£o de random para pegar uma resposta randomica da nossa lista\n",
    "#             result = random.choice(idx['responses'])\n",
    "#             break\n",
    "#     return result\n",
    "\n",
    "def conversa(msg):    \n",
    "    ints = class_prediction(msg, ourNewModel)\n",
    "    res = get_response(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b85469",
   "metadata": {
    "id": "a9b85469",
    "outputId": "444f31dd-85e1-483d-b407-f7a94e51ee8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mande uma mensagem ao nosso bot, ele reconhece pedidos relacionadas a disciplinas e atende sauda√ß√µes!\n"
     ]
    }
   ],
   "source": [
    "print(\"mande uma mensagem ao nosso bot, ele reconhece pedidos relacionadas a disciplinas e atende sauda√ß√µes!\")\n",
    "print(conversa(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ecaaa1",
   "metadata": {
    "id": "c4ecaaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 1.1630e-08 - precision: 1.0000\n",
      "test loss, test acc: [1.1630173268883937e-08, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = ourNewModel.evaluate(X_test, y_test, batch_size=1)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
